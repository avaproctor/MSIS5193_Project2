# -*- coding: utf-8 -*-
"""Project2-Question1&3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11_2zSTt6uqa7k5gN63Lhn6cGQhr0Ke7W
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %pip install langchain-ollama

import streamlit as st
import ollama

#Title
st.title("Input to AI")

#My Question Input Box
Question = st.text_input("Enter your question:")
#My File Uploading Box
Attachment = st.file_uploader("Upload attachment:")

#Title for AI Repsonse
st.title("AI Response:")

#For Inputted Items
if Question:
    Context = ""

    #Read uploaded document
    if Attachment is not None:
        Context = Attachment.read().decode("utf-8", errors="ignore")

    #Response to Inputted Items
    if Context:
        st.write("The AI is using the uploaded document as context.")
    else:
        st.write(
            "I'm happy to play along.\n\n"
            "Since the context is empty, I'll answer directly.\n\n"
        )

    #Open-Source LLM
    prompt = (
        "Use the following document as context if provided.\n\n"
        f"Document:\n{Context}\n\n"
        f"Question:\n{Question}"
    )

    #Ollama Response
    response = ollama.chat(
        model="llama3",
        messages=[
            {"role": "user", "content": prompt}
        ]
    )

    #Display Ollama AI answer
    st.write(response["message"]["content"])

#Question 3 Deployment Bonus:

#   **Insert the Streamlit link here**